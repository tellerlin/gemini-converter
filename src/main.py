# src/main.py - å®Œæ•´ç‰ˆæœ¬
import asyncio
import time
import os
from typing import Dict, Optional, Any, Set, AsyncGenerator, Union, List
from contextlib import asynccontextmanager
from dataclasses import dataclass
from enum import Enum

from fastapi import FastAPI, HTTPException, Request, Depends, Body
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.security import APIKeyHeader, HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger
from dotenv import load_dotenv
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

# å¯¼å…¥æ–°çš„ openai_adapter æ¨¡å—
from src.openai_adapter import (
    ChatCompletionRequest, APIConfig
)
from src.config import get_config
from src.performance import initialize_performance_modules, get_performance_stats, monitor_performance

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- å¯†é’¥çŠ¶æ€ä¸ä¿¡æ¯ ---
class KeyStatus(Enum):
    ACTIVE = "active"
    COOLING = "cooling"
    FAILED = "failed"

@dataclass
class APIKeyInfo:
    key: str
    status: KeyStatus = KeyStatus.ACTIVE
    failure_count: int = 0
    cooling_until: Optional[float] = None
    last_used: Optional[float] = None
    total_requests: int = 0
    successful_requests: int = 0

# --- ä¾èµ–æ³¨å…¥ä¸å®‰å…¨ ---
config = get_config()
api_config: Optional[APIConfig] = None
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)
bearer_scheme = HTTPBearer(auto_error=False)
valid_api_keys: Set[str] = set(config.SECURITY_ADAPTER_API_KEYS)
admin_api_keys: Set[str] = set(config.SECURITY_ADMIN_API_KEYS)

async def verify_api_key(
    api_key: Optional[str] = Depends(api_key_header), 
    bearer: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme)
):
    """éªŒè¯APIå¯†é’¥ï¼Œæ”¯æŒX-API-Keyå¤´å’ŒBearer Tokenä¸¤ç§æ–¹å¼"""
    if not valid_api_keys:
        # åœ¨ä¸å®‰å…¨çš„æ¨¡å¼ä¸‹å…è®¸æ‰€æœ‰è¯·æ±‚
        logger.warning("Running in insecure mode - no API keys configured")
        return "insecure_mode"
    
    key = api_key or (bearer.credentials if bearer else None)
    if key and key in valid_api_keys:
        return key
    
    raise HTTPException(
        status_code=401, 
        detail="Invalid API Key or Bearer Token. Use X-API-Key header or Authorization: Bearer <token>"
    )

async def verify_admin_key(
    api_key: Optional[str] = Depends(api_key_header), 
    bearer: Optional[HTTPAuthorizationCredentials] = Depends(bearer_scheme)
):
    """éªŒè¯ç®¡ç†å‘˜APIå¯†é’¥"""
    if not admin_api_keys:
        raise HTTPException(status_code=403, detail="Admin API keys not configured")
    
    key = api_key or (bearer.credentials if bearer else None)
    if key and key in admin_api_keys:
        return key
    
    raise HTTPException(status_code=403, detail="Invalid Admin API Key")

# --- å¢å¼ºçš„ Gemini å¯†é’¥ç®¡ç†å™¨ ---
class GeminiKeyManager:
    def __init__(self):
        self.keys: Dict[str, APIKeyInfo] = {
            key: APIKeyInfo(key=key) for key in config.GEMINI_API_KEYS if key
        }
        self.lock = asyncio.Lock()
        self.last_used_key_index = -1
        
        if not self.keys:
            raise ValueError("No valid GEMINI_API_KEYS provided.")
        
        logger.info(f"Initialized {len(self.keys)} Gemini API keys.")

    async def get_available_key(self) -> Optional[APIKeyInfo]:
        """è·å–å¯ç”¨çš„APIå¯†é’¥ï¼Œä½¿ç”¨æ™ºèƒ½è½®è¯¢ç­–ç•¥"""
        async with self.lock:
            self._recover_keys()
            active_keys = [k for k in self.keys.values() if k.status == KeyStatus.ACTIVE]
            
            if not active_keys:
                logger.warning("No active Gemini API keys available")
                return None
            
            # æ™ºèƒ½è½®è¯¢ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹©æœ€è¿‘æœªä½¿ç”¨çš„å¯†é’¥
            now = time.time()
            active_keys.sort(key=lambda k: k.last_used or 0)
            
            # å¦‚æœæœ‰ä»æœªä½¿ç”¨çš„å¯†é’¥ï¼Œä¼˜å…ˆä½¿ç”¨
            unused_keys = [k for k in active_keys if k.last_used is None]
            if unused_keys:
                selected_key = unused_keys[0]
            else:
                # ä½¿ç”¨è½®è¯¢ç­–ç•¥
                self.last_used_key_index = (self.last_used_key_index + 1) % len(active_keys)
                selected_key = active_keys[self.last_used_key_index]
            
            # æ›´æ–°ä½¿ç”¨è®°å½•
            selected_key.last_used = now
            selected_key.total_requests += 1
            
            logger.debug(f"Selected key: {selected_key.key[:8]}... (total requests: {selected_key.total_requests})")
            return selected_key

    def _recover_keys(self):
        """æ¢å¤å†·å´ä¸­çš„å¯†é’¥"""
        now = time.time()
        recovered_count = 0
        
        for key_info in self.keys.values():
            if (key_info.status == KeyStatus.COOLING and 
                key_info.cooling_until and 
                now > key_info.cooling_until):
                
                key_info.status = KeyStatus.ACTIVE
                key_info.cooling_until = None
                recovered_count += 1
                logger.info(f"Key {key_info.key[:8]}... recovered to ACTIVE.")
        
        if recovered_count > 0:
            logger.info(f"Recovered {recovered_count} keys from cooling state")

    async def mark_key_success(self, key: str):
        """æ ‡è®°å¯†é’¥ä½¿ç”¨æˆåŠŸ"""
        async with self.lock:
            if key in self.keys:
                key_info = self.keys[key]
                key_info.successful_requests += 1
                # æˆåŠŸè¯·æ±‚åé‡ç½®å¤±è´¥è®¡æ•°ï¼ˆéƒ¨åˆ†é‡ç½®ï¼‰
                if key_info.failure_count > 0:
                    key_info.failure_count = max(0, key_info.failure_count - 1)

    async def mark_key_failed(self, key: str, error: Exception):
        """æ ‡è®°å¯†é’¥å¤±è´¥å¹¶è¿›å…¥å†·å´æˆ–æ°¸ä¹…å¤±è´¥çŠ¶æ€"""
        async with self.lock:
            if key not in self.keys:
                logger.warning(f"Attempt to mark unknown key as failed: {key[:8]}...")
                return
            
            key_info = self.keys[key]
            key_info.failure_count += 1
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ°¸ä¹…æ€§é”™è¯¯
            is_permanent = isinstance(error, (
                google_exceptions.PermissionDenied, 
                google_exceptions.Unauthenticated,
                google_exceptions.InvalidArgument
            ))
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºä¸´æ—¶æ€§é”™è¯¯ï¼ˆå¦‚é…é¢é™åˆ¶ï¼‰
            is_quota_error = isinstance(error, google_exceptions.ResourceExhausted)
            
            if is_permanent:
                key_info.status = KeyStatus.FAILED
                status_msg = "permanently FAILED"
            elif is_quota_error:
                # é…é¢é”™è¯¯ä½¿ç”¨æ›´é•¿çš„å†·å´æ—¶é—´
                key_info.status = KeyStatus.COOLING
                key_info.cooling_until = time.time() + (config.GEMINI_COOLING_PERIOD * 3)
                status_msg = f"COOLING (quota) for {config.GEMINI_COOLING_PERIOD * 3}s"
            elif key_info.failure_count >= config.GEMINI_MAX_RETRIES:
                key_info.status = KeyStatus.FAILED
                status_msg = "permanently FAILED (max retries exceeded)"
            else:
                key_info.status = KeyStatus.COOLING
                # æŒ‡æ•°é€€é¿å†·å´æ—¶é—´
                cooling_time = config.GEMINI_COOLING_PERIOD * (2 ** (key_info.failure_count - 1))
                key_info.cooling_until = time.time() + min(cooling_time, 3600)  # æœ€å¤š1å°æ—¶
                status_msg = f"COOLING for {min(cooling_time, 3600)}s"
            
            logger.warning(
                f"Key {key_info.key[:8]}... marked as {status_msg}. "
                f"Failure count: {key_info.failure_count}. "
                f"Success rate: {key_info.successful_requests}/{key_info.total_requests}. "
                f"Reason: {type(error).__name__}: {str(error)}"
            )
    
    async def get_stats(self) -> Dict[str, int]:
        """è·å–å¯†é’¥ä½¿ç”¨ç»Ÿè®¡"""
        async with self.lock:
            self._recover_keys()
            return {
                "total": len(self.keys),
                "active": sum(1 for k in self.keys.values() if k.status == KeyStatus.ACTIVE),
                "cooling": sum(1 for k in self.keys.values() if k.status == KeyStatus.COOLING),
                "failed": sum(1 for k in self.keys.values() if k.status == KeyStatus.FAILED),
            }

    async def get_detailed_stats(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„å¯†é’¥ç»Ÿè®¡ä¿¡æ¯"""
        async with self.lock:
            self._recover_keys()
            
            total_requests = sum(k.total_requests for k in self.keys.values())
            total_successful = sum(k.successful_requests for k in self.keys.values())
            
            return {
                "summary": await self.get_stats(),
                "performance": {
                    "total_requests": total_requests,
                    "successful_requests": total_successful,
                    "success_rate": total_successful / total_requests if total_requests > 0 else 0,
                    "average_requests_per_key": total_requests / len(self.keys) if self.keys else 0
                },
                "keys": [
                    {
                        "key_id": key[:8] + "..." + key[-4:],
                        "status": info.status.value,
                        "failure_count": info.failure_count,
                        "total_requests": info.total_requests,
                        "successful_requests": info.successful_requests,
                        "success_rate": info.successful_requests / info.total_requests if info.total_requests > 0 else 0,
                        "cooling_until": info.cooling_until,
                        "cooling_remaining": max(0, (info.cooling_until or 0) - time.time()) if info.cooling_until else 0,
                        "last_used": info.last_used
                    }
                    for key, info in self.keys.items()
                ]
            }

    async def add_key(self, key: str) -> bool:
        """åŠ¨æ€æ·»åŠ æ–°çš„APIå¯†é’¥"""
        async with self.lock:
            if key in self.keys:
                return False  # å¯†é’¥å·²å­˜åœ¨
            
            self.keys[key] = APIKeyInfo(key=key)
            logger.info(f"Added new API key: {key[:8]}...")
            return True

    async def remove_key(self, key: str) -> bool:
        """åŠ¨æ€ç§»é™¤APIå¯†é’¥"""
        async with self.lock:
            if key not in self.keys:
                return False  # å¯†é’¥ä¸å­˜åœ¨
            
            del self.keys[key]
            logger.info(f"Removed API key: {key[:8]}...")
            return True

    async def update_key_status(self, key: str, status: KeyStatus) -> bool:
        """æ›´æ–°å¯†é’¥çŠ¶æ€"""
        async with self.lock:
            if key not in self.keys:
                return False
            
            self.keys[key].status = status
            if status == KeyStatus.ACTIVE:
                self.keys[key].cooling_until = None
                self.keys[key].failure_count = 0
            
            logger.info(f"Updated key {key[:8]}... status to {status.value}")
            return True

key_manager: Optional[GeminiKeyManager] = None

# --- å¢å¼ºçš„ OpenAI é£æ ¼çš„ Gemini é€‚é…å™¨ ---
class OAIStyleGeminiAdapter:
    def __init__(self, key_mgr: GeminiKeyManager, api_cfg: APIConfig):
        self.key_manager = key_mgr
        self.api_config = api_cfg

    def _validate_request(self, request: ChatCompletionRequest) -> Optional[str]:
        """éªŒè¯è¯·æ±‚å‚æ•°ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯æˆ–None"""
        try:
            # éªŒè¯æ¶ˆæ¯
            if not request.messages:
                return "Messages array cannot be empty"
            
            # éªŒè¯å·¥å…·å®šä¹‰
            if request.tools:
                for i, tool in enumerate(request.tools):
                    if not isinstance(tool, dict):
                        return f"Tool {i} must be a dictionary"
                    
                    if tool.get("type") != "function":
                        return f"Tool {i} type must be 'function'"
                    
                    function = tool.get("function", {})
                    if not function.get("name"):
                        return f"Tool {i} function must have a name"
                    
                    # éªŒè¯å‚æ•°schema
                    parameters = function.get("parameters", {})
                    if not isinstance(parameters, dict):
                        return f"Tool {i} parameters must be a dictionary"
            
            # éªŒè¯æ¸©åº¦å‚æ•°
            if not (0.0 <= request.temperature <= 2.0):
                return "Temperature must be between 0.0 and 2.0"
            
            # éªŒè¯top_på‚æ•°
            if request.top_p is not None and not (0.0 <= request.top_p <= 1.0):
                return "top_p must be between 0.0 and 1.0"
            
            # éªŒè¯max_tokens
            if request.max_tokens is not None and request.max_tokens <= 0:
                return "max_tokens must be positive"
            
            # éªŒè¯nå‚æ•°
            if hasattr(request, 'n') and request.n is not None and (request.n < 1 or request.n > 10):
                return "n must be between 1 and 10"
            
            return None
            
        except Exception as e:
            return f"Request validation error: {str(e)}"

    async def process_chat_completion(self, request: ChatCompletionRequest) -> Union[Dict, AsyncGenerator[str, None]]:
        """å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚ï¼Œæ”¯æŒå¢å¼ºçš„é‡è¯•æœºåˆ¶å’Œå·¥å…·è°ƒç”¨"""
        
        # éªŒè¯è¯·æ±‚
        validation_error = self._validate_request(request)
        if validation_error:
            raise HTTPException(status_code=400, detail=validation_error)
        
        last_error = None
        max_attempts = min(config.GEMINI_MAX_RETRIES + 1, len(self.key_manager.keys))
        
        candidate_count = getattr(request, 'n', 1) or 1
        
        logger.info(f"Processing chat completion request for model: {request.model}, stream: {request.stream}, tools: {len(request.tools) if request.tools else 0}, n: {candidate_count}")
        
        for attempt in range(max_attempts):
            key_info = await self.key_manager.get_available_key()
            if not key_info:
                logger.error("No available Gemini API keys. All keys are cooling or have failed.")
                if attempt < max_attempts - 1:
                    await asyncio.sleep(min(5 * (attempt + 1), 30))  # æŒ‡æ•°é€€é¿ç­‰å¾…
                    continue
                else:
                    break

            logger.info(f"Attempt {attempt + 1}/{max_attempts} using key {key_info.key[:8]}... (req: {key_info.total_requests})")
            
            try:
                # é…ç½®Gemini API
                genai.configure(api_key=key_info.key)
                
                # è½¬æ¢è¯·æ±‚æ ¼å¼
                messages, system_prompt = self.api_config.openai_to_gemini.convert_messages(request.messages)
                if not messages:
                    raise HTTPException(status_code=400, detail="No valid messages after conversion")
                
                # è½¬æ¢å·¥å…·å®šä¹‰å’Œå·¥å…·é€‰æ‹©
                tools = None
                tool_config = None
                if request.tools:
                    try:
                        tools, tool_config = self.api_config.openai_to_gemini.convert_tools(request.tools, request.tool_choice)
                        logger.debug(f"Converted {len(request.tools)} tools to Gemini format with tool_config: {tool_config}")
                    except Exception as tool_error:
                        logger.error(f"Error converting tools: {tool_error}")
                        raise HTTPException(status_code=400, detail=f"Tool conversion error: {str(tool_error)}")
                
                model_name = self.api_config.openai_to_gemini.convert_model(request.model)
                logger.debug(f"Converted to Gemini: model={model_name}, messages={len(messages)}, tools={len(tools) if tools else 0}")

                # åˆ›å»ºç”Ÿæˆæ¨¡å‹
                model_kwargs = {
                    "model_name": model_name,
                    "system_instruction": system_prompt,
                }
                
                if tools:
                    model_kwargs["tools"] = tools

                model = genai.GenerativeModel(**model_kwargs)
                
                # å¢å¼ºçš„ç”Ÿæˆé…ç½®ï¼ŒåŒ…å«top_på’Œresponse_formatæ”¯æŒ
                generation_config_kwargs = {
                    "max_output_tokens": min(request.max_tokens or 8192, 8192),
                    "temperature": request.temperature,
                    "candidate_count": candidate_count,  # æ”¯æŒå¤šå€™é€‰å›å¤
                }
                
                # æ·»åŠ top_pæ”¯æŒ
                if request.top_p is not None:
                    generation_config_kwargs["top_p"] = request.top_p
                
                # æ·»åŠ JSONè¾“å‡ºæ ¼å¼æ”¯æŒ
                if request.response_format and request.response_format.get("type") == "json_object":
                    generation_config_kwargs["response_mime_type"] = "application/json"
                    logger.debug("Enabled JSON response format")
                
                generation_config = genai.types.GenerationConfig(**generation_config_kwargs)
                
                # è¯·æ±‚é…ç½®ï¼ŒåŒ…å«tool_config
                request_options = {
                    'timeout': config.GEMINI_REQUEST_TIMEOUT
                }
                
                # APIè°ƒç”¨å‚æ•°
                api_call_kwargs = {
                    "generation_config": generation_config,
                    "request_options": request_options
                }
                
                # æ·»åŠ tool_configæ”¯æŒ
                if tool_config:
                    api_call_kwargs["tool_config"] = tool_config
                    logger.debug(f"Using tool_config: {tool_config}")
                
                if request.stream:
                    if candidate_count > 1:
                        raise HTTPException(status_code=400, detail="Streaming is not supported when n > 1.")
                        
                    # æµå¼å“åº”
                    logger.debug("Creating streaming response")
                    try:
                        stream = model.generate_content_async(
                            messages, 
                            stream=True, 
                            **api_call_kwargs
                        )
                        
                        # åŒ…è£…æµå¼å“åº”ä»¥å¤„ç†æˆåŠŸ/å¤±è´¥
                        async def wrapped_stream():
                            try:
                                async for chunk in self.api_config.gemini_to_openai.convert_stream_response(stream, request):
                                    yield chunk
                                # å¦‚æœæµå®Œæˆæ²¡æœ‰å¼‚å¸¸ï¼Œæ ‡è®°æˆåŠŸ
                                await self.key_manager.mark_key_success(key_info.key)
                            except Exception as stream_error:
                                await self.key_manager.mark_key_failed(key_info.key, stream_error)
                                raise
                        
                        return wrapped_stream()
                        
                    except Exception as stream_setup_error:
                        logger.error(f"Error setting up stream: {stream_setup_error}")
                        await self.key_manager.mark_key_failed(key_info.key, stream_setup_error)
                        raise
                else:
                    # éæµå¼å“åº”
                    logger.debug("Creating non-streaming response")
                    response = await model.generate_content_async(
                        messages, 
                        **api_call_kwargs
                    )
                    
                    # æ£€æŸ¥å“åº”æœ‰æ•ˆæ€§
                    if not response.candidates:
                        raise ValueError("No candidates in Gemini response")
                    
                    # æ ‡è®°æˆåŠŸå¹¶è½¬æ¢å“åº”
                    await self.key_manager.mark_key_success(key_info.key)
                    converted_response = self.api_config.gemini_to_openai.convert_response(response, request)
                    
                    return converted_response
            
            except google_exceptions.ResourceExhausted as quota_error:
                last_error = quota_error
                logger.warning(f"Quota exhausted for key {key_info.key[:8]}...: {quota_error}")
                await self.key_manager.mark_key_failed(key_info.key, quota_error)
                continue
                
            except google_exceptions.InvalidArgument as arg_error:
                last_error = arg_error
                logger.error(f"Invalid argument for key {key_info.key[:8]}...: {arg_error}")
                await self.key_manager.mark_key_failed(key_info.key, arg_error)
                # å‚æ•°é”™è¯¯é€šå¸¸æ˜¯è¯·æ±‚æœ¬èº«çš„é—®é¢˜ï¼Œä¸éœ€è¦é‡è¯•å…¶ä»–å¯†é’¥
                raise HTTPException(status_code=400, detail=f"Invalid request: {str(arg_error)}")
                
            except google_exceptions.PermissionDenied as perm_error:
                last_error = perm_error
                logger.error(f"Permission denied for key {key_info.key[:8]}...: {perm_error}")
                await self.key_manager.mark_key_failed(key_info.key, perm_error)
                continue
                
            except google_exceptions.Unauthenticated as auth_error:
                last_error = auth_error
                logger.error(f"Authentication failed for key {key_info.key[:8]}...: {auth_error}")
                await self.key_manager.mark_key_failed(key_info.key, auth_error)
                continue
                
            except asyncio.TimeoutError as timeout_error:
                last_error = timeout_error
                logger.warning(f"Timeout for key {key_info.key[:8]}...: {timeout_error}")
                await self.key_manager.mark_key_failed(key_info.key, timeout_error)
                continue
                
            except Exception as e:
                last_error = e
                error_msg = f"Attempt {attempt+1} failed with key {key_info.key[:8]}. Error: {type(e).__name__}: {str(e)}"
                logger.warning(error_msg)
                
                await self.key_manager.mark_key_failed(key_info.key, e)
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                if attempt < max_attempts - 1:
                    wait_time = min(2 ** attempt, 30)  # æœ€å¤šç­‰å¾…30ç§’
                    logger.info(f"Waiting {wait_time}s before retry...")
                    await asyncio.sleep(wait_time)
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
        if isinstance(last_error, google_exceptions.ResourceExhausted):
            detail = "All API keys have reached their quota limits. Please try again later."
            status_code = 429  # Too Many Requests
        elif isinstance(last_error, google_exceptions.PermissionDenied):
            detail = "All API keys lack necessary permissions."
            status_code = 403  # Forbidden
        elif isinstance(last_error, google_exceptions.Unauthenticated):
            detail = "All API keys are invalid or expired."
            status_code = 401  # Unauthorized
        else:
            detail = f"All {max_attempts} attempts failed. Last error: {type(last_error).__name__}: {str(last_error)}"
            status_code = 502  # Bad Gateway
            
        logger.error(detail)
        raise HTTPException(status_code=status_code, detail=detail)

adapter: Optional[OAIStyleGeminiAdapter] = None

# --- FastAPI åº”ç”¨ç”Ÿå‘½å‘¨æœŸ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global key_manager, adapter, api_config
    
    # å¯åŠ¨æ—¶çš„åˆå§‹åŒ–
    logger.info("ğŸš€ Starting OpenAI-Style Gemini Adapter...")
    app.state.start_time = time.time()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs("logs", exist_ok=True)
    logger.add(
        "logs/adapter_{time}.log", 
        rotation="1 day", 
        retention="7 days", 
        level="INFO",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
    )
    
    try:
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§æ¨¡å—
        initialize_performance_modules(
            cache_enabled=config.CACHE_ENABLED,
            cache_max_size=config.CACHE_MAX_SIZE,
            cache_ttl=config.CACHE_TTL,
            cache_key_prefix=config.CACHE_KEY_PREFIX
        )
        logger.info("âœ… Performance monitoring initialized")
        
        # åˆå§‹åŒ–APIé…ç½®å’Œé€‚é…å™¨
        api_config = APIConfig()
        logger.info("âœ… API configuration initialized")
        
        key_manager = GeminiKeyManager()
        logger.info("âœ… Gemini key manager initialized")
        
        adapter = OAIStyleGeminiAdapter(key_manager, api_config)
        logger.info("âœ… OpenAI-Style Gemini Adapter initialized")
        
        # è¾“å‡ºå¯åŠ¨ä¿¡æ¯
        stats = await key_manager.get_stats()
        logger.info(f"ğŸ”‘ API Keys: {stats['active']} active, {stats['cooling']} cooling, {stats['failed']} failed")
        logger.info("ğŸ¯ OpenAI-Style Gemini Adapter started successfully!")
        
        yield
        
    except Exception as e:
        logger.critical(f"âŒ Application failed to start: {e}", exc_info=True)
        raise
    finally:
        # å…³é—­æ—¶çš„æ¸…ç†
        logger.info("ğŸ›‘ Adapter shutting down...")

# --- FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(
    title="OpenAI-Style Gemini Adapter",
    description="""
    Advanced bridge between OpenAI API and Google's Gemini Pro with enhanced features.
    
    ## Features
    - ğŸ”„ Intelligent API key rotation with success tracking
    - ğŸš€ Streaming and non-streaming responses
    - ğŸ› ï¸ Enhanced function/tool calling support with validation
    - ğŸ“Š Comprehensive performance monitoring
    - ğŸ”’ Secure API key management with detailed statistics
    - ğŸŒ Full OpenAI API compatibility
    - âš¡ Optimized error handling and recovery
    - ğŸ¯ Smart quota management and exponential backoff
    - ğŸ”§ Dynamic key management for runtime flexibility
    - ğŸ›ï¸ Advanced generation parameters (top_p, JSON format, multiple candidates)
    
    ## Authentication
    Use either:
    - `X-API-Key` header
    - `Authorization: Bearer <token>` header
    
    ## Error Handling
    - Automatic retry with exponential backoff
    - Smart key rotation on failures
    - Detailed error reporting and logging
    """,
    version="4.2.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=config.SERVICE_CORS_ORIGINS, # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# --- ä¸»è¦ç«¯ç‚¹ ---

@app.post("/v1/chat/completions", 
          summary="Create chat completion", 
          description="Create a chat completion with enhanced tool support, compatible with OpenAI API")
async def create_chat_completion(
    request: ChatCompletionRequest, 
    client_key: str = Depends(verify_api_key)
):
    """OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹ï¼Œæ”¯æŒå¢å¼ºçš„å·¥å…·è°ƒç”¨"""
    if not adapter:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    try:
        async with monitor_performance("create_chat_completion"):
            response = await adapter.process_chat_completion(request)
            
            if request.stream:
                return StreamingResponse(
                    response, 
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                        "Content-Type": "text/event-stream",
                        "Access-Control-Allow-Origin": "*",
                        "X-Accel-Buffering": "no",  # ç¦ç”¨nginxç¼“å†²
                    }
                )
            else:
                return JSONResponse(content=response)
                
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in chat completion: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.get("/v1/models", 
         summary="List models", 
         description="List available models in OpenAI format with accurate metadata")
async def list_models(client_key: str = Depends(verify_api_key)):
    """OpenAIå…¼å®¹çš„æ¨¡å‹åˆ—è¡¨ç«¯ç‚¹ï¼Œæä¾›å‡†ç¡®çš„æ¨¡å‹ä¿¡æ¯"""
    current_time = int(time.time())
    # ä¿®å¤ï¼šæä¾›å‡†ç¡®çš„max_tokenså¹¶å®Œæˆåˆ—è¡¨
    model_data = [
        {
            "id": "gpt-4o",
            "object": "model",
            "created": current_time,
            "owned_by": "openai-emulated",
            "permission": [],
            "root": "gpt-4o",
            "parent": None,
            "context_window": 1048576,
            "max_tokens": 8192, # Gemini 1.5 Pro max output is 8192
            "capabilities": ["chat", "tools", "streaming", "json_mode", "vision"]
        },
        {
            "id": "gpt-4-turbo",
            "object": "model",
            "created": current_time,
            "owned_by": "openai-emulated",
            "permission": [],
            "root": "gpt-4-turbo",
            "parent": None,
            "context_window": 1048576,
            "max_tokens": 8192,
            "capabilities": ["chat", "tools", "streaming", "json_mode", "vision"]
        },
        {
            "id": "gpt-4o-mini",
            "object": "model",
            "created": current_time,
            "owned_by": "openai-emulated",
            "permission": [],
            "root": "gpt-4o-mini",
            "parent": None,
            "context_window": 1048576,
            "max_tokens": 8192, # Gemini 1.5 Flash max output is 8192
            "capabilities": ["chat", "tools", "streaming", "json_mode", "vision"]
        },
        {
            "id": "gpt-3.5-turbo",
            "object": "model",
            "created": current_time,
            "owned_by": "openai-emulated",
            "permission": [],
            "root": "gpt-3.5-turbo",
            "parent": None,
            "context_window": 1048576,
            "max_tokens": 8192,
            "capabilities": ["chat", "tools", "streaming", "json_mode", "vision"]
        },
    ]
    return {"object": "list", "data": model_data}

# --- å¥åº·æ£€æŸ¥å’Œç›‘æ§ç«¯ç‚¹ ---

@app.get("/health", 
         summary="Health check", 
         description="Check service health and key availability with detailed status")
async def health_check():
    """å¢å¼ºçš„å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    if not key_manager:
        raise HTTPException(status_code=503, detail="Key Manager not initialized")
    
    stats = await key_manager.get_stats()
    detailed_stats = await key_manager.get_detailed_stats()
    
    is_healthy = stats["active"] > 0
    status_code = 200 if is_healthy else 503
    
    health_data = {
        "status": "healthy" if is_healthy else "degraded",
        "timestamp": int(time.time()),
        "service": "OpenAI-Style Gemini Adapter",
        "version": "4.2.0",
        "key_summary": stats,
        "performance": detailed_stats["performance"],
        "uptime": time.time() - getattr(app.state, 'start_time', time.time()),
        "message": "All systems operational" if is_healthy else "Some API keys unavailable"
    }
    
    return JSONResponse(content=health_data, status_code=status_code)

@app.get("/stats", 
         summary="Get statistics", 
         description="Get comprehensive service performance and key usage statistics")
async def get_stats(client_key: str = Depends(verify_api_key)):
    """å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯ç«¯ç‚¹"""
    if not key_manager:
        raise HTTPException(status_code=503, detail="Key Manager not initialized")

    key_stats = await key_manager.get_detailed_stats()
    perf_stats = get_performance_stats()

    # Combinar todas las estadÃ­sticas en una sola respuesta
    all_stats = {
        "key_management_stats": key_stats,
        "adapter_performance_stats": perf_stats.get("performance_stats", {}),
        "cache_stats": perf_stats.get("cache_stats", {})
    }

    return JSONResponse(content=all_stats)


# --- æ–°å¢ï¼šåŠ¨æ€å¯†é’¥ç®¡ç†ç«¯ç‚¹ ---

@app.post("/admin/keys", 
          summary="Add a new Gemini API key", 
          tags=["Admin Key Management"],
          status_code=201)
async def add_gemini_key(
    admin_key: str = Depends(verify_admin_key),
    key_to_add: str = Body(..., embed=True, description="The Gemini API key to add.")
):
    """åŠ¨æ€æ·»åŠ ä¸€ä¸ªæ–°çš„Gemini APIå¯†é’¥åˆ°æ± ä¸­ã€‚"""
    if not key_manager:
        raise HTTPException(status_code=503, detail="Key Manager not initialized")
    
    success = await key_manager.add_key(key_to_add)
    if not success:
        raise HTTPException(status_code=409, detail="API key already exists.")
    
    return {"status": "success", "message": f"API key starting with {key_to_add[:8]} added."}


@app.delete("/admin/keys", 
            summary="Remove a Gemini API key", 
            tags=["Admin Key Management"],
            status_code=200)
async def remove_gemini_key(
    admin_key: str = Depends(verify_admin_key),
    key_to_remove: str = Body(..., embed=True, description="The Gemini API key to remove.")
):
    """ä»æ± ä¸­åŠ¨æ€ç§»é™¤ä¸€ä¸ªæŒ‡å®šçš„Gemini APIå¯†é’¥ã€‚"""
    if not key_manager:
        raise HTTPException(status_code=503, detail="Key Manager not initialized")

    success = await key_manager.remove_key(key_to_remove)
    if not success:
        raise HTTPException(status_code=404, detail="API key not found.")
    
    return {"status": "success", "message": f"API key starting with {key_to_remove[:8]} removed."}


@app.put("/admin/keys/{key_id}", 
         summary="Update status of a Gemini API key", 
         tags=["Admin Key Management"],
         status_code=200)
async def update_gemini_key_status(
    key_id: str,
    status: KeyStatus,
    admin_key: str = Depends(verify_admin_key),
):
    """æ‰‹åŠ¨æ›´æ–°ä¸€ä¸ªå¯†é’¥çš„çŠ¶æ€ï¼ˆä¾‹å¦‚ï¼Œå°†ä¸€ä¸ªå†·å´ä¸­çš„å¯†é’¥é‡ç½®ä¸ºæ¿€æ´»çŠ¶æ€ï¼‰ã€‚"""
    if not key_manager:
        raise HTTPException(status_code=503, detail="Key Manager not initialized")
    
    # å› ä¸ºæˆ‘ä»¬åªå­˜å‚¨äº†éƒ¨åˆ†keyä¿¡æ¯ï¼Œéœ€è¦æ‰¾åˆ°å®Œæ•´çš„key
    full_key = None
    for k in key_manager.keys.keys():
        if k.startswith(key_id):
            full_key = k
            break
            
    if not full_key:
         raise HTTPException(status_code=404, detail=f"No key found starting with '{key_id}'")

    success = await key_manager.update_key_status(full_key, status)
    if not success:
        # This case is unlikely if the above check passes, but for completeness
        raise HTTPException(status_code=404, detail="API key not found for status update.")

    return {"status": "success", "message": f"Status of key {full_key[:8]}... updated to {status.value}."}

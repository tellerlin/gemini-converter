# src/openai_adapter.py
import json
import uuid
import time
from typing import List, Dict, Optional, Any, Union, AsyncGenerator, Literal, Tuple

from pydantic import BaseModel, Field, validator
import logging

import google.generativeai as genai
from google.generativeai.types import GenerationConfig, ContentDict, PartDict, Tool as GeminiTool, FunctionDeclaration

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ========== OpenAI API æ•°æ®æ¨¡å‹ ==========
class ToolCall(BaseModel):
    id: str
    type: Literal["function"] = "function"
    function: Dict[str, Any]

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant", "tool"]
    content: Union[str, List[Dict[str, Any]], None] = None
    name: Optional[str] = None
    tool_call_id: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None

    @validator('content', pre=True)
    def validate_content(cls, v, values):
        role = values.get('role')
        # toolè§’è‰²çš„æ¶ˆæ¯å¿…é¡»æœ‰content
        if role == 'tool' and not v:
            raise ValueError("Tool messages must have content")
        return v

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    max_tokens: Optional[int] = 4096
    stream: bool = False
    temperature: float = Field(default=1.0, ge=0.0, le=2.0)
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = None
    parallel_tool_calls: bool = True

    class Config:
        extra = 'allow'

    @validator('tools')
    def validate_tools(cls, v):
        if v is not None:
            for tool in v:
                if not isinstance(tool, dict):
                    raise ValueError("Each tool must be a dictionary")
                if tool.get("type") != "function":
                    raise ValueError("Only function tools are supported")
                function = tool.get("function", {})
                if not function.get("name"):
                    raise ValueError("Function must have a name")
        return v


# ========== è½¬æ¢å™¨ç±» ==========

class OpenAIToGeminiConverter:
    """
    å°† OpenAI API è¯·æ±‚æ ¼å¼è½¬æ¢ä¸º Google Gemini API æ ¼å¼ã€‚
    """
    
    def convert_model(self, openai_model: str) -> str:
        """å°†OpenAIæ¨¡å‹åç§°æ˜ å°„åˆ°Geminiæ¨¡å‹åç§°"""
        model_map = {
            "gpt-4o": "gemini-1.5-pro-latest",
            "gpt-4o-mini": "gemini-1.5-flash-latest",
            "gpt-4-turbo": "gemini-1.5-pro-latest",
            "gpt-4": "gemini-1.5-pro-latest",
            "gpt-3.5-turbo": "gemini-1.5-flash-latest",
        }
        mapped_model = model_map.get(openai_model, "gemini-1.5-pro-latest")
        logger.debug(f"Mapped model {openai_model} -> {mapped_model}")
        return mapped_model

    def _convert_schema_to_gemini(self, json_schema: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’åœ°å°†OpenAIçš„JSON Schemaè½¬æ¢ä¸ºGeminiçš„æ ¼å¼ã€‚"""
        if not isinstance(json_schema, dict):
            logger.warning("Schema is not a dictionary, returning empty schema")
            return {"type": "OBJECT", "properties": {}}
            
        type_mapping = {
            "string": "STRING",
            "number": "NUMBER", 
            "integer": "INTEGER",
            "boolean": "BOOLEAN",
            "object": "OBJECT",
            "array": "ARRAY",
            "null": "STRING"  # Gemini doesn't have null type, treat as string
        }
        
        schema_type = json_schema.get("type", "object")
        gemini_type = type_mapping.get(schema_type, "STRING")
        
        if gemini_type not in type_mapping.values():
            logger.warning(f"Unknown schema type: {schema_type}, defaulting to STRING")
            gemini_type = "STRING"

        gemini_schema = {"type": gemini_type}
        
        # å¤åˆ¶åŸºæœ¬å±æ€§
        for key in ["description"]:
            if key in json_schema and json_schema[key]:
                gemini_schema[key] = str(json_schema[key])
        
        # å¤„ç†æšä¸¾
        if "enum" in json_schema and isinstance(json_schema["enum"], list):
            gemini_schema["enum"] = [str(item) for item in json_schema["enum"]]
        
        # å¤„ç†å¯¹è±¡ç±»å‹
        if gemini_type == "OBJECT":
            properties = json_schema.get("properties", {})
            if isinstance(properties, dict):
                gemini_schema["properties"] = {
                    k: self._convert_schema_to_gemini(v)
                    for k, v in properties.items()
                    if isinstance(v, dict)
                }
            
            required = json_schema.get("required", [])
            if isinstance(required, list):
                gemini_schema["required"] = [str(item) for item in required]
        
        # å¤„ç†æ•°ç»„ç±»å‹
        elif gemini_type == "ARRAY":
            items = json_schema.get("items")
            if isinstance(items, dict):
                gemini_schema["items"] = self._convert_schema_to_gemini(items)
            else:
                # é»˜è®¤æ•°ç»„é¡¹ç±»å‹
                gemini_schema["items"] = {"type": "STRING"}
            
        return gemini_schema

    def convert_tools(self, tools: Optional[List[Dict]], tool_choice: Optional[Union[str, Dict]] = None) -> Optional[List[GeminiTool]]:
        """å°†OpenAIå·¥å…·å®šä¹‰è½¬æ¢ä¸ºGeminiå·¥å…·å®šä¹‰"""
        if not tools:
            return None
        
        gemini_functions = []
        for i, tool in enumerate(tools):
            try:
                if not isinstance(tool, dict):
                    logger.warning(f"Tool {i} is not a dictionary, skipping")
                    continue
                    
                if tool.get("type") != "function":
                    logger.warning(f"Tool {i} is not a function type, skipping")
                    continue
                
                func_info = tool.get("function", {})
                if not isinstance(func_info, dict):
                    logger.warning(f"Tool {i} function info is not a dictionary, skipping")
                    continue
                    
                name = func_info.get("name")
                if not name or not isinstance(name, str):
                    logger.warning(f"Tool {i} missing valid name, skipping")
                    continue
                
                description = func_info.get("description", "")
                if not isinstance(description, str):
                    description = str(description) if description else ""
                
                parameters = func_info.get("parameters", {"type": "object", "properties": {}})
                if not isinstance(parameters, dict):
                    logger.warning(f"Function {name} has invalid parameters, using empty schema")
                    parameters = {"type": "object", "properties": {}}

                # åˆ›å»ºå‡½æ•°å£°æ˜
                try:
                    function_declaration = FunctionDeclaration(
                        name=name,
                        description=description,
                        parameters=self._convert_schema_to_gemini(parameters)
                    )
                    gemini_functions.append(function_declaration)
                    logger.debug(f"Successfully converted function: {name}")
                except Exception as e:
                    logger.error(f"Error creating FunctionDeclaration for {name}: {e}")
                    continue
                    
            except Exception as e:
                logger.error(f"Error converting tool {i}: {e}")
                continue
        
        if gemini_functions:
            logger.info(f"Converted {len(gemini_functions)} functions to Gemini format")
            return [GeminiTool(function_declarations=gemini_functions)]
        
        logger.warning("No valid functions found in tools")
        return None

    def convert_messages(self, messages: List[ChatMessage]) -> Tuple[List[ContentDict], Optional[str]]:
        """å°†OpenAIæ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºGeminiæ¶ˆæ¯æ ¼å¼"""
        if not messages:
            logger.warning("No messages provided")
            return [], None
            
        gemini_messages = []
        system_prompt = None
        
        # é¦–å…ˆæå–ç³»ç»Ÿæ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.role == "system"]
        if system_messages:
            system_contents = []
            for msg in system_messages:
                if isinstance(msg.content, str) and msg.content.strip():
                    system_contents.append(msg.content.strip())
            if system_contents:
                system_prompt = "\n\n".join(system_contents)
                logger.debug(f"Extracted system prompt: {system_prompt[:100]}...")
        
        # ç„¶åå¤„ç†éç³»ç»Ÿæ¶ˆæ¯
        non_system_messages = [msg for msg in messages if msg.role != "system"]
        
        for i, msg in enumerate(non_system_messages):
            try:
                # ç¡®å®šè§’è‰²æ˜ å°„
                if msg.role == "user":
                    role = "user"
                elif msg.role in ["assistant", "tool"]:
                    role = "model"
                else:
                    logger.warning(f"Unknown role {msg.role} in message {i}, treating as user")
                    role = "user"
                
                parts = []
                
                # å¤„ç†æ™®é€šæ–‡æœ¬å†…å®¹
                if isinstance(msg.content, str) and msg.content:
                    parts.append(PartDict(text=msg.content))
                elif isinstance(msg.content, list):
                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                    for content_part in msg.content:
                        if isinstance(content_part, dict):
                            if content_part.get("type") == "text":
                                text = content_part.get("text", "")
                                if text:
                                    parts.append(PartDict(text=str(text)))
                            elif content_part.get("type") == "image_url":
                                # Geminiæ”¯æŒå›¾ç‰‡ï¼Œä½†éœ€è¦ä¸åŒçš„æ ¼å¼
                                logger.warning("Image content not yet supported in this converter")

                # å¤„ç†assistantçš„å·¥å…·è°ƒç”¨
                if msg.role == "assistant" and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        try:
                            if not isinstance(tool_call, (dict, ToolCall)):
                                logger.warning("Invalid tool_call format")
                                continue
                                
                            if isinstance(tool_call, dict):
                                tool_call_dict = tool_call
                            else:
                                tool_call_dict = tool_call.dict()
                            
                            if tool_call_dict.get("type") == "function":
                                func_info = tool_call_dict.get("function", {})
                                function_name = func_info.get("name")
                                arguments = func_info.get("arguments", "{}")
                                
                                if not function_name:
                                    logger.warning("Tool call missing function name")
                                    continue
                                
                                # è§£æå‚æ•°
                                try:
                                    if isinstance(arguments, str):
                                        parsed_args = json.loads(arguments)
                                    elif isinstance(arguments, dict):
                                        parsed_args = arguments
                                    else:
                                        parsed_args = {}
                                except json.JSONDecodeError as e:
                                    logger.error(f"Invalid JSON in tool call arguments: {e}")
                                    parsed_args = {}
                                
                                parts.append(PartDict(
                                    function_call={
                                        'name': function_name, 
                                        'args': parsed_args
                                    }
                                ))
                                logger.debug(f"Added function call: {function_name}")
                                
                        except Exception as e:
                            logger.error(f"Error processing tool call: {e}")
                            continue

                # å¤„ç†toolè§’è‰²çš„å“åº”
                if msg.role == "tool":
                    function_name = msg.name or "unknown_function"
                    content = msg.content or ""
                    
                    # å°è¯•è§£æcontentä¸ºJSONï¼Œå¦‚æœå¤±è´¥åˆ™ä½œä¸ºå­—ç¬¦ä¸²å¤„ç†
                    try:
                        if isinstance(content, str):
                            # å°è¯•è§£æä¸ºJSON
                            try:
                                parsed_content = json.loads(content)
                            except json.JSONDecodeError:
                                parsed_content = content
                        else:
                            parsed_content = content
                    except Exception:
                        parsed_content = str(content)
                    
                    parts.append(PartDict(
                        function_response={
                            'name': function_name, 
                            'response': parsed_content
                        }
                    ))
                    logger.debug(f"Added function response: {function_name}")

                # å¦‚æœæ²¡æœ‰å†…å®¹ä½†æœ‰è§’è‰²ï¼Œæ·»åŠ ç©ºæ–‡æœ¬
                if not parts and role == "user":
                    parts.append(PartDict(text=""))

                # æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
                if parts:
                    gemini_messages.append(ContentDict(role=role, parts=parts))
                    logger.debug(f"Converted message {i}: role={role}, parts_count={len(parts)}")
                    
            except Exception as e:
                logger.error(f"Error converting message {i}: {e}")
                continue

        logger.info(f"Converted {len(messages)} OpenAI messages to {len(gemini_messages)} Gemini messages")
        return gemini_messages, system_prompt


class GeminiToOpenAIConverter:
    """
    å°† Gemini API å“åº”æ ¼å¼è½¬æ¢ä¸º OpenAI API å“åº”æ ¼å¼ã€‚
    """
    
    def _map_finish_reason(self, reason) -> str:
        """æ˜ å°„Geminiçš„ç»“æŸåŸå› åˆ°OpenAIæ ¼å¼"""
        if not reason:
            return "stop"
            
        reason_str = str(reason).upper()
        if "MAX_TOKENS" in reason_str or "LENGTH" in reason_str:
            return "length"
        elif "TOOL" in reason_str or "FUNCTION" in reason_str:
            return "tool_calls"
        elif "SAFETY" in reason_str:
            return "content_filter"
        elif "STOP" in reason_str:
            return "stop"
        else:
            return "stop"

    def convert_response(self, gemini_response: genai.types.GenerateContentResponse, original_request: ChatCompletionRequest) -> Dict[str, Any]:
        """å°†Geminiå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        model = original_request.model
        finish_reason = "stop"
        tool_calls = []
        text_content = ""

        try:
            if gemini_response.candidates and len(gemini_response.candidates) > 0:
                candidate = gemini_response.candidates[0]
                
                # å¤„ç†å®ŒæˆåŸå› 
                if hasattr(candidate, 'finish_reason') and candidate.finish_reason:
                    finish_reason = self._map_finish_reason(candidate.finish_reason)
                
                # å¤„ç†å†…å®¹
                if candidate.content and candidate.content.parts:
                    for part in candidate.content.parts:
                        # å¤„ç†æ–‡æœ¬å†…å®¹
                        if hasattr(part, 'text') and part.text:
                            text_content += part.text
                        
                        # å¤„ç†å·¥å…·è°ƒç”¨
                        if hasattr(part, 'function_call') and part.function_call:
                            try:
                                tool_call_id = f"call_{uuid.uuid4().hex}"
                                function_name = part.function_call.name
                                function_args = dict(part.function_call.args) if part.function_call.args else {}
                                
                                tool_calls.append({
                                    "id": tool_call_id,
                                    "type": "function",
                                    "function": {
                                        "name": function_name,
                                        "arguments": json.dumps(function_args, ensure_ascii=False)
                                    }
                                })
                                logger.debug(f"Converted function call: {function_name}")
                            except Exception as e:
                                logger.error(f"Error processing function call: {e}")
                                continue

            # æ„å»ºæ¶ˆæ¯å¯¹è±¡
            message = {
                "role": "assistant",
                "content": text_content if text_content else None,
            }
            
            # åªæœ‰åœ¨æœ‰å·¥å…·è°ƒç”¨æ—¶æ‰æ·»åŠ tool_callså­—æ®µ
            if tool_calls:
                message["tool_calls"] = tool_calls
                if not text_content:
                    message["content"] = None  # æ˜ç¡®è®¾ç½®ä¸ºNoneå½“åªæœ‰å·¥å…·è°ƒç”¨æ—¶

            choice = {
                "index": 0,
                "message": message,
                "finish_reason": finish_reason
            }

            # å¤„ç†ä½¿ç”¨ç»Ÿè®¡
            usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            if hasattr(gemini_response, 'usage_metadata') and gemini_response.usage_metadata:
                usage = {
                    "prompt_tokens": getattr(gemini_response.usage_metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(gemini_response.usage_metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(gemini_response.usage_metadata, 'total_token_count', 0)
                }

            response = {
                "id": f"chatcmpl-{uuid.uuid4().hex}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [choice],
                "usage": usage,
                "system_fingerprint": None
            }
            
            logger.debug(f"Converted response: finish_reason={finish_reason}, tool_calls={len(tool_calls)}, content_length={len(text_content)}")
            return response
            
        except Exception as e:
            logger.error(f"Error converting Gemini response: {e}")
            # è¿”å›é”™è¯¯å“åº”
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"Error processing response: {str(e)}"
                    },
                    "finish_reason": "stop"
                }],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "system_fingerprint": None
            }

    async def convert_stream_response(self, gemini_stream, original_request: ChatCompletionRequest) -> AsyncGenerator[str, None]:
        """å°†Geminiæµå¼å“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        model = original_request.model
        chat_id = f"chatcmpl-{uuid.uuid4().hex}"
        created_time = int(time.time())

        # ç”¨äºè·Ÿè¸ªå·¥å…·è°ƒç”¨çš„çŠ¶æ€
        active_tool_calls = {}
        tool_call_index = 0
        first_chunk_sent = False
        content_buffer = ""

        try:
            async for chunk in gemini_stream:
                try:
                    if not chunk.candidates or len(chunk.candidates) == 0:
                        continue
                    
                    candidate = chunk.candidates[0]
                    delta = {}
                    
                    # å‘é€åˆå§‹è§’è‰²å—
                    if not first_chunk_sent:
                        delta["role"] = "assistant"
                        first_chunk_sent = True
                        
                        choice = {"index": 0, "delta": delta}
                        openai_chunk = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": model,
                            "choices": [choice]
                        }
                        yield f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"
                        continue
                    
                    # å¤„ç†å†…å®¹
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            # å¤„ç†æ–‡æœ¬å†…å®¹
                            if hasattr(part, 'text') and part.text:
                                new_content = part.text
                                # åªå‘é€æ–°å¢çš„å†…å®¹
                                if new_content != content_buffer:
                                    delta_content = new_content[len(content_buffer):]
                                    if delta_content:
                                        delta["content"] = delta_content
                                        content_buffer = new_content

                            # å¤„ç†å·¥å…·è°ƒç”¨
                            if hasattr(part, 'function_call') and part.function_call:
                                function_name = part.function_call.name
                                function_args = dict(part.function_call.args) if part.function_call.args else {}
                                
                                # ä¸ºæ–°çš„å·¥å…·è°ƒç”¨åˆ›å»ºæ¡ç›®
                                if function_name not in active_tool_calls:
                                    tool_call_id = f"call_{uuid.uuid4().hex}"
                                    active_tool_calls[function_name] = {
                                        "index": tool_call_index,
                                        "id": tool_call_id,
                                        "type": "function",
                                        "function": {
                                            "name": function_name,
                                            "arguments": ""
                                        }
                                    }
                                    tool_call_index += 1
                                    
                                    # å‘é€å·¥å…·è°ƒç”¨å¼€å§‹
                                    delta["tool_calls"] = [{
                                        "index": active_tool_calls[function_name]["index"],
                                        "id": tool_call_id,
                                        "type": "function",
                                        "function": {"name": function_name, "arguments": ""}
                                    }]
                                
                                # æ›´æ–°å‚æ•°ï¼ˆå¢é‡æ–¹å¼ï¼‰
                                new_args = json.dumps(function_args, ensure_ascii=False)
                                current_args = active_tool_calls[function_name]["function"]["arguments"]
                                
                                if new_args != current_args:
                                    delta_args = new_args[len(current_args):]
                                    if delta_args:
                                        active_tool_calls[function_name]["function"]["arguments"] = new_args
                                        delta["tool_calls"] = [{
                                            "index": active_tool_calls[function_name]["index"],
                                            "function": {"arguments": delta_args}
                                        }]
                    
                    # å‘é€å†…å®¹å—
                    if delta:
                        choice = {"index": 0, "delta": delta}
                        openai_chunk = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": model,
                            "choices": [choice]
                        }
                        yield f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"

                    # å¤„ç†ç»“æŸåŸå› 
                    if (hasattr(candidate, 'finish_reason') and 
                        candidate.finish_reason and 
                        candidate.finish_reason.name != "FINISH_REASON_UNSPECIFIED"):
                        
                        finish_reason = self._map_finish_reason(candidate.finish_reason)
                        final_choice = {
                            "index": 0, 
                            "delta": {}, 
                            "finish_reason": finish_reason
                        }
                        final_chunk = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": model,
                            "choices": [final_choice]
                        }
                        yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
                        break
                        
                except Exception as chunk_error:
                    logger.error(f"Error processing stream chunk: {chunk_error}")
                    # å‘é€é”™è¯¯å†…å®¹ä½†ç»§ç»­å¤„ç†
                    error_delta = {"content": f"[Error processing chunk: {str(chunk_error)}]"}
                    error_choice = {"index": 0, "delta": error_delta}
                    error_chunk = {
                        "id": chat_id,
                        "object": "chat.completion.chunk", 
                        "created": created_time,
                        "model": model,
                        "choices": [error_choice]
                    }
                    yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
                    continue

        except Exception as stream_error:
            logger.error(f"Error in stream conversion: {stream_error}")
            # å‘é€æœ€ç»ˆé”™è¯¯å—
            error_chunk = {
                "id": chat_id,
                "object": "chat.completion.chunk",
                "created": created_time,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": f"\n\n[Stream Error: {str(stream_error)}]"},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"

        # å‘é€æµç»“æŸæ ‡å¿—
        yield "data: [DONE]\n\n"


class APIConfig:
    """æŒæœ‰è½¬æ¢å™¨å®ä¾‹çš„é…ç½®ç±»ã€‚"""
    def __init__(self):
        self.openai_to_gemini = OpenAIToGeminiConverter()
        self.gemini_to_openai = GeminiToOpenAIConverter()
        logger.info("ğŸš€ OpenAI-Gemini API Compatibility Layer Initialized (Enhanced Version).")

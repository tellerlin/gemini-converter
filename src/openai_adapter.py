# src/openai_adapter.py - ä¿®å¤ç‰ˆæœ¬
import json
import uuid
import time
import re
from typing import List, Dict, Optional, Any, Union, AsyncGenerator, Literal, Tuple

from pydantic import BaseModel, Field, field_validator, model_validator
import logging

import google.generativeai as genai
from google.generativeai.types import GenerationConfig, ContentDict, PartDict, Tool as GeminiTool, FunctionDeclaration
from google.ai.generativelanguage_v1beta.types import content as glm_content

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ¨¡å‹æ˜ å°„é…ç½®ï¼ˆå¤–éƒ¨åŒ–ï¼‰
MODEL_MAPPING = {
    "gpt-4o": "gemini-1.5-pro-latest",
    "gpt-4o-mini": "gemini-1.5-flash-latest",
    "gpt-4-turbo": "gemini-1.5-pro-latest",
    "gpt-4": "gemini-1.5-pro-latest",
    "gpt-3.5-turbo": "gemini-1.5-flash-latest",
    "gpt-4-1106-preview": "gemini-1.5-pro-latest",
    "gpt-4-0125-preview": "gemini-1.5-pro-latest",
    "gpt-3.5-turbo-1106": "gemini-1.5-flash-latest",
}

# ========== OpenAI API æ•°æ®æ¨¡å‹ - å¢å¼ºç‰ˆ ==========
class ToolFunction(BaseModel):
    name: str
    arguments: str

class ToolCall(BaseModel):
    id: str
    type: Literal["function"] = "function"
    function: ToolFunction

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant", "tool"]
    content: Union[str, List[Dict[str, Any]], None] = None
    name: Optional[str] = None
    tool_call_id: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None

    @field_validator('content', mode='before')
    @classmethod
    def validate_content(cls, v, values):
        """éªŒè¯æ¶ˆæ¯å†…å®¹çš„æœ‰æ•ˆæ€§"""
        if hasattr(values, 'data'):
            role = values.data.get('role') if hasattr(values, 'data') else values.get('role')
        else:
            role = values.get('role')
        # toolè§’è‰²çš„æ¶ˆæ¯å¿…é¡»æœ‰content
        if role == 'tool' and not v:
            raise ValueError("Tool messages must have content")
        return v

    @field_validator('tool_calls', mode='before')
    @classmethod
    def validate_tool_calls(cls, v, values):
        """éªŒè¯å·¥å…·è°ƒç”¨åªèƒ½åœ¨assistantæ¶ˆæ¯ä¸­ä½¿ç”¨"""
        if v is not None:
            if hasattr(values, 'data'):
                role = values.data.get('role') if hasattr(values, 'data') else values.get('role')
            else:
                role = values.get('role')
            if role != 'assistant':
                raise ValueError("Only assistant messages can have tool_calls")
        return v

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    max_tokens: Optional[int] = 4096
    stream: bool = False
    temperature: float = Field(default=1.0, ge=0.0, le=2.0)
    top_p: Optional[float] = Field(default=None, ge=0.0, le=1.0)
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = None
    parallel_tool_calls: bool = True
    response_format: Optional[Dict[str, Any]] = None

    class Config:
        extra = 'allow'

    @field_validator('tools', mode='before')
    @classmethod
    def validate_tools(cls, v):
        if v is not None:
            for i, tool in enumerate(v):
                if not isinstance(tool, dict):
                    raise ValueError(f"Tool {i} must be a dictionary")
                if tool.get("type") != "function":
                    raise ValueError(f"Tool {i}: Only function tools are supported")
                function = tool.get("function", {})
                if not function.get("name"):
                    raise ValueError(f"Tool {i}: Function must have a name")
                # éªŒè¯å‚æ•°schema
                parameters = function.get("parameters", {})
                if not isinstance(parameters, dict):
                    raise ValueError(f"Tool {i}: Function parameters must be a dictionary")
        return v

    @model_validator(mode='after')
    def validate_tools_and_choice(self):
        """éªŒè¯å·¥å…·å’Œå·¥å…·é€‰æ‹©çš„ä¸€è‡´æ€§"""
        if self.tool_choice is not None and not self.tools:
            raise ValueError("tool_choice can only be set when tools are provided")
        return self


# ========== è½¬æ¢å™¨ç±» - ä¿®å¤ç‰ˆ ==========

class OpenAIToGeminiConverter:
    """
    å°† OpenAI API è¯·æ±‚æ ¼å¼è½¬æ¢ä¸º Google Gemini API æ ¼å¼ã€‚
    ä¿®å¤äº†å·¥å…·è°ƒç”¨å’Œæ¶ˆæ¯è½¬æ¢çš„å„ç§é—®é¢˜ã€‚
    """

    def convert_model(self, openai_model: str) -> str:
        """å°†OpenAIæ¨¡å‹åç§°æ˜ å°„åˆ°Geminiæ¨¡å‹åç§°ï¼ˆä½¿ç”¨å¤–éƒ¨é…ç½®ï¼‰"""
        mapped_model = MODEL_MAPPING.get(openai_model, "gemini-1.5-pro-latest")
        logger.debug(f"Mapped model {openai_model} -> {mapped_model}")
        return mapped_model

    def _convert_schema_to_gemini(self, json_schema: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’åœ°å°†OpenAIçš„JSON Schemaè½¬æ¢ä¸ºGeminiçš„æ ¼å¼ï¼ˆä¿®å¤äº†nullç±»å‹å¤„ç†ï¼‰ã€‚"""
        if not isinstance(json_schema, dict):
            logger.warning("Schema is not a dictionary, returning empty schema")
            return {"type": "OBJECT", "properties": {}}

        # æ›´ç²¾ç¡®çš„ç±»å‹æ˜ å°„
        type_mapping = {
            "string": "STRING",
            "number": "NUMBER",
            "integer": "INTEGER",
            "boolean": "BOOLEAN",
            "object": "OBJECT",
            "array": "ARRAY",
            # ä¿®å¤ï¼šå°†nullç±»å‹æ˜ å°„ä¸ºSTRINGï¼Œä½†æ·»åŠ ç‰¹æ®Šæ ‡è®°
            "null": "STRING"
        }

        schema_type = json_schema.get("type", "object").lower()
        gemini_type = type_mapping.get(schema_type, "STRING")

        gemini_schema = {"type": gemini_type}

        # å¤åˆ¶åŸºæœ¬å±æ€§
        if "description" in json_schema and json_schema["description"]:
            description = str(json_schema["description"])
            # ä¿®å¤ï¼šä¸ºnullç±»å‹æ·»åŠ ç‰¹æ®Šè¯´æ˜
            if schema_type == "null":
                description += " (Note: null values should be represented as empty strings)"
            gemini_schema["description"] = description
        elif schema_type == "null":
            gemini_schema["description"] = "Null value (use empty string)"

        # å¤„ç†æ ¼å¼é™åˆ¶
        if "format" in json_schema:
            gemini_schema["format"] = str(json_schema["format"])

        # å¤„ç†æšä¸¾ï¼ˆä¿®å¤ï¼šä¸ºnullç±»å‹ç‰¹æ®Šå¤„ç†ï¼‰
        if "enum" in json_schema and isinstance(json_schema["enum"], list):
            enum_values = []
            for item in json_schema["enum"]:
                if item is None:
                    enum_values.append("")  # å°†nullè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
                else:
                    enum_values.append(str(item))
            gemini_schema["enum"] = enum_values

        # å¤„ç†æ•°å€¼èŒƒå›´
        for key in ["minimum", "maximum", "minLength", "maxLength"]:
            if key in json_schema and isinstance(json_schema[key], (int, float)):
                gemini_schema[key.lower()] = json_schema[key]

        # å¤„ç†å¯¹è±¡ç±»å‹
        if gemini_type == "OBJECT":
            properties = json_schema.get("properties", {})
            if isinstance(properties, dict):
                gemini_schema["properties"] = {
                    k: self._convert_schema_to_gemini(v)
                    for k, v in properties.items()
                    if isinstance(v, dict)
                }
            else:
                gemini_schema["properties"] = {}

            required = json_schema.get("required", [])
            if isinstance(required, list) and required:
                gemini_schema["required"] = [str(item) for item in required]

        # å¤„ç†æ•°ç»„ç±»å‹
        elif gemini_type == "ARRAY":
            items = json_schema.get("items")
            if isinstance(items, dict):
                gemini_schema["items"] = self._convert_schema_to_gemini(items)
            elif isinstance(items, list) and items:
                # å¦‚æœitemsæ˜¯æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªä½œä¸ºæ¨¡æ¿
                gemini_schema["items"] = self._convert_schema_to_gemini(items[0])
            else:
                # é»˜è®¤æ•°ç»„é¡¹ç±»å‹
                gemini_schema["items"] = {"type": "STRING"}

        return gemini_schema

    def _convert_tool_choice_to_tool_config(self, tool_choice: Optional[Union[str, Dict[str, Any]]]) -> Optional[Dict[str, Any]]:
        """å°†OpenAIçš„tool_choiceè½¬æ¢ä¸ºGeminiçš„tool_config"""
        if tool_choice is None or tool_choice == "auto":
            return {"function_calling_config": {"mode": "AUTO"}}

        if tool_choice == "none":
            return {"function_calling_config": {"mode": "NONE"}}
        
        if tool_choice == "required":
             # Gemini doesn't have a direct equivalent for "required" like OpenAI.
             # "ANY" is the closest, as it allows the model to decide which function to call.
            logger.warning("OpenAI 'tool_choice: required' is mapped to Gemini 'mode: ANY'.")
            return {"function_calling_config": {"mode": "ANY"}}

        # å¤„ç†æŒ‡å®šç‰¹å®šå‡½æ•°çš„æƒ…å†µ
        if isinstance(tool_choice, dict):
            if tool_choice.get("type") == "function":
                function_name = tool_choice.get("function", {}).get("name")
                if function_name:
                    return {
                        "function_calling_config": {
                            "mode": "ANY",
                            "allowed_function_names": [function_name]
                        }
                    }

        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ä¸”ä¸æ˜¯ "auto" æˆ– "none"ï¼Œå‡è®¾æ˜¯å‡½æ•°å
        if isinstance(tool_choice, str) and tool_choice not in ["auto", "none"]:
            return {
                "function_calling_config": {
                    "mode": "ANY",
                    "allowed_function_names": [tool_choice]
                }
            }

        logger.warning(f"Unsupported tool_choice format: {tool_choice}, using AUTO")
        return {"function_calling_config": {"mode": "AUTO"}}

    def convert_tools(self, tools: Optional[List[Dict]], tool_choice: Optional[Union[str, Dict[str, Any]]] = None) -> Tuple[Optional[List[GeminiTool]], Optional[Dict[str, Any]]]:
        """
        å°†OpenAIå·¥å…·å®šä¹‰è½¬æ¢ä¸ºGeminiå·¥å…·å®šä¹‰å’Œå·¥å…·é…ç½®
        ä¿®å¤ï¼šç°åœ¨æ­£ç¡®å¤„ç†tool_choiceå‚æ•°
        è¿”å›ï¼š(gemini_tools, tool_config)
        """
        if not tools:
            return None, None

        gemini_functions = []
        for i, tool in enumerate(tools):
            try:
                if not isinstance(tool, dict):
                    logger.warning(f"Tool {i} is not a dictionary, skipping")
                    continue

                if tool.get("type") != "function":
                    logger.warning(f"Tool {i} is not a function type (got: {tool.get('type')}), skipping")
                    continue

                func_info = tool.get("function", {})
                if not isinstance(func_info, dict):
                    logger.warning(f"Tool {i} function info is not a dictionary, skipping")
                    continue

                name = func_info.get("name")
                if not name or not isinstance(name, str) or not name.strip():
                    logger.warning(f"Tool {i} missing valid name, skipping")
                    continue

                # ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„å‡½æ•°åç§°æ ¼å¼éªŒè¯ï¼ˆåªå…è®¸å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼‰
                if not re.match(r'^[a-zA-Z0-9_]+$', name):
                    logger.warning(f"Tool {i} has invalid function name format: {name}. Only alphanumeric characters and underscores are allowed.")
                    continue

                description = func_info.get("description", "")
                if not isinstance(description, str):
                    description = str(description) if description else f"Function: {name}"

                parameters = func_info.get("parameters", {"type": "object", "properties": {}})
                if not isinstance(parameters, dict):
                    logger.warning(f"Function {name} has invalid parameters, using empty schema")
                    parameters = {"type": "object", "properties": {}}

                # åˆ›å»ºå‡½æ•°å£°æ˜ï¼Œæ·»åŠ æ›´å¥½çš„é”™è¯¯å¤„ç†
                try:
                    converted_params = self._convert_schema_to_gemini(parameters)

                    function_declaration = FunctionDeclaration(
                        name=name,
                        description=description or f"Function: {name}",
                        parameters=converted_params
                    )
                    gemini_functions.append(function_declaration)
                    logger.debug(f"Successfully converted function: {name}")

                except Exception as e:
                    logger.error(f"Error creating FunctionDeclaration for {name}: {e}")
                    logger.debug(f"Function parameters: {parameters}")
                    continue

            except Exception as e:
                logger.error(f"Error converting tool {i}: {e}")
                continue

        gemini_tools = None
        tool_config = self._convert_tool_choice_to_tool_config(tool_choice)

        if gemini_functions:
            logger.info(f"Converted {len(gemini_functions)} out of {len(tools)} functions to Gemini format")
            gemini_tools = [GeminiTool(function_declarations=gemini_functions)]
            logger.debug(f"Converted tool_choice {tool_choice} to tool_config: {tool_config}")
        else:
            logger.warning("No valid functions found in tools")

        return gemini_tools, tool_config

    def convert_messages(self, messages: List[ChatMessage]) -> Tuple[List[ContentDict], Optional[str]]:
        """å°†OpenAIæ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºGeminiæ¶ˆæ¯æ ¼å¼ï¼Œä¼˜åŒ–å·¥å…·è°ƒç”¨å¤„ç†"""
        if not messages:
            logger.warning("No messages provided")
            return [], None

        gemini_messages = []
        system_prompt = None

        # é¦–å…ˆæå–ç³»ç»Ÿæ¶ˆæ¯
        system_messages = [msg for msg in messages if msg.role == "system"]
        if system_messages:
            system_contents = []
            for msg in system_messages:
                if isinstance(msg.content, str) and msg.content.strip():
                    system_contents.append(msg.content.strip())
                elif isinstance(msg.content, list):
                    # å¤„ç†å¤šæ¨¡æ€ç³»ç»Ÿæ¶ˆæ¯
                    for content_part in msg.content:
                        if isinstance(content_part, dict) and content_part.get("type") == "text":
                            text = content_part.get("text", "")
                            if text and text.strip():
                                system_contents.append(text.strip())

            if system_contents:
                system_prompt = "\n\n".join(system_contents)
                logger.debug(f"Extracted system prompt: {system_prompt[:100]}...")

        # å¤„ç†éç³»ç»Ÿæ¶ˆæ¯
        non_system_messages = [msg for msg in messages if msg.role != "system"]

        for i, msg in enumerate(non_system_messages):
            try:
                # ç¡®å®šè§’è‰²æ˜ å°„
                if msg.role == "user":
                    role = "user"
                elif msg.role in ["assistant", "tool"]:
                    role = "model"
                else:
                    logger.warning(f"Unknown role {msg.role} in message {i}, treating as user")
                    role = "user"

                parts = []

                # å¤„ç†æ™®é€šæ–‡æœ¬å†…å®¹
                if isinstance(msg.content, str) and msg.content:
                    parts.append(PartDict(text=msg.content))
                elif isinstance(msg.content, list):
                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼ˆä¼˜åŒ–ï¼‰
                    for content_part in msg.content:
                        if isinstance(content_part, dict):
                            content_type = content_part.get("type")
                            if content_type == "text":
                                text = content_part.get("text", "")
                                if text:
                                    parts.append(PartDict(text=str(text)))
                            elif content_type == "image_url":
                                # æ”¯æŒå›¾ç‰‡å†…å®¹ï¼ˆGemini 1.5æ”¯æŒï¼‰
                                image_url = content_part.get("image_url", {})
                                url = image_url.get("url", "")
                                if url:
                                    if url.startswith("data:image"):
                                        # Base64å›¾ç‰‡
                                        try:
                                            import base64
                                            # è§£ædata URL
                                            header, data = url.split(",", 1)
                                            base64.b64decode(data) # Just to validate
                                            parts.append(PartDict(inline_data={
                                                "mime_type": header.split(";")[0].split(":")[1],
                                                "data": data
                                            }))
                                        except Exception as e:
                                            logger.warning(f"Failed to process base64 image: {e}")
                                    else:
                                        # URLå›¾ç‰‡ï¼ˆæ³¨æ„ï¼šGeminiå¯èƒ½ä¸æ”¯æŒå¤–éƒ¨URLï¼‰
                                        logger.warning("External image URLs may not be supported by Gemini")

                # å¤„ç†assistantçš„å·¥å…·è°ƒç”¨ï¼ˆä¼˜åŒ–ï¼‰
                if msg.role == "assistant" and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        try:
                            if isinstance(tool_call, dict):
                                # å¤„ç†å­—å…¸æ ¼å¼çš„tool_call
                                func_info = tool_call.get("function", {})
                            elif hasattr(tool_call, 'function'):
                                # å¤„ç†ToolCallå¯¹è±¡
                                func_info = {
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments
                                }
                            else:
                                logger.warning("Invalid tool_call format")
                                continue

                            function_name = func_info.get("name")
                            arguments = func_info.get("arguments", "{}")

                            if not function_name:
                                logger.warning("Tool call missing function name")
                                continue

                            # è§£æå‚æ•°ï¼ˆæ›´å¥å£®çš„å¤„ç†ï¼‰
                            try:
                                if isinstance(arguments, str):
                                    if arguments.strip():
                                        parsed_args = json.loads(arguments)
                                    else:
                                        parsed_args = {}
                                elif isinstance(arguments, dict):
                                    parsed_args = arguments
                                else:
                                    logger.warning(f"Unexpected arguments type: {type(arguments)}")
                                    parsed_args = {}
                            except json.JSONDecodeError as e:
                                logger.error(f"Invalid JSON in tool call arguments for {function_name}: {e}")
                                logger.debug(f"Arguments string: {arguments}")
                                parsed_args = {}

                            parts.append(PartDict(
                                function_call=glm_content.FunctionCall(
                                    name=function_name,
                                    args=parsed_args
                                )
                            ))
                            logger.debug(f"Added function call: {function_name} with args: {list(parsed_args.keys())}")

                        except Exception as e:
                            logger.error(f"Error processing tool call: {e}")
                            continue

                # å¤„ç†toolè§’è‰²çš„å“åº”ï¼ˆä¼˜åŒ–ï¼‰
                if msg.role == "tool":
                    function_name = msg.name or "unknown_function"
                    content = msg.content or ""

                    # å¤„ç†å·¥å…·å“åº”å†…å®¹
                    try:
                        if isinstance(content, str):
                            # å°è¯•è§£æä¸ºJSONï¼Œä½†ä¿ç•™åŸå§‹å­—ç¬¦ä¸²ä½œä¸ºå¤‡é€‰
                            try:
                                parsed_content = json.loads(content)
                                response_content = parsed_content
                            except json.JSONDecodeError:
                                response_content = {"result": content}
                        elif isinstance(content, (dict, list)):
                            response_content = content
                        else:
                            response_content = {"result": str(content)}
                    except Exception:
                        response_content = {"result": str(content)}

                    parts.append(PartDict(
                        function_response=glm_content.FunctionResponse(
                            name=function_name,
                            response=response_content
                        )
                    ))
                    logger.debug(f"Added function response: {function_name}")

                # å¦‚æœæ²¡æœ‰å†…å®¹ä½†æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ ç©ºæ–‡æœ¬
                if not parts and role == "user":
                    parts.append(PartDict(text=""))

                # æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
                if parts:
                    gemini_messages.append(ContentDict(role=role, parts=parts))
                    logger.debug(f"Converted message {i}: role={role}, parts_count={len(parts)}")
                elif role == "user":
                    # ç”¨æˆ·æ¶ˆæ¯å³ä½¿ä¸ºç©ºä¹Ÿè¦æ·»åŠ 
                    gemini_messages.append(ContentDict(role=role, parts=[PartDict(text="")]))

            except Exception as e:
                logger.error(f"Error converting message {i}: {e}")
                continue

        logger.info(f"Converted {len(messages)} OpenAI messages to {len(gemini_messages)} Gemini messages")
        return gemini_messages, system_prompt


class GeminiToOpenAIConverter:
    """
    å°† Gemini API å“åº”æ ¼å¼è½¬æ¢ä¸º OpenAI API å“åº”æ ¼å¼ã€‚
    ä¿®å¤äº†æµå¼å·¥å…·è°ƒç”¨å‚æ•°èšåˆçš„ä¸¥é‡é—®é¢˜ã€‚
    """

    def _map_finish_reason(self, reason) -> str:
        """æ˜ å°„Geminiçš„ç»“æŸåŸå› åˆ°OpenAIæ ¼å¼"""
        if not reason:
            return "stop"

        reason_str = str(reason).upper()
        if "MAX_TOKENS" in reason_str or "LENGTH" in reason_str:
            return "length"
        elif "TOOL_CALLS" in reason_str or "TOOL" in reason_str or "FUNCTION" in reason_str:
            return "tool_calls"
        elif "SAFETY" in reason_str or "BLOCKED" in reason_str:
            return "content_filter"
        elif "STOP" in reason_str or "FINISH" in reason_str:
            return "stop"
        else:
            return "stop"

    def convert_response(self, gemini_response: genai.types.GenerateContentResponse, original_request: ChatCompletionRequest) -> Dict[str, Any]:
        """å°†Geminiå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼ï¼Œä¼˜åŒ–å·¥å…·è°ƒç”¨å¤„ç†"""
        model = original_request.model
        finish_reason = "stop"
        tool_calls = []
        text_content = ""

        try:
            if gemini_response.candidates and len(gemini_response.candidates) > 0:
                candidate = gemini_response.candidates[0]

                # å¤„ç†å®ŒæˆåŸå› 
                if hasattr(candidate, 'finish_reason') and candidate.finish_reason:
                    finish_reason = self._map_finish_reason(candidate.finish_reason)

                # å¤„ç†å†…å®¹
                if candidate.content and candidate.content.parts:
                    for part in candidate.content.parts:
                        # å¤„ç†æ–‡æœ¬å†…å®¹
                        if hasattr(part, 'text') and part.text:
                            text_content += part.text

                        # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆä¼˜åŒ–ï¼‰
                        if hasattr(part, 'function_call') and part.function_call:
                            try:
                                tool_call_id = f"call_{uuid.uuid4().hex}"
                                function_name = part.function_call.name

                                # æ›´å®‰å…¨çš„å‚æ•°å¤„ç†
                                function_args = {}
                                if hasattr(part.function_call, 'args') and part.function_call.args:
                                    try:
                                        # Geminiçš„argså¯èƒ½æ˜¯å¤šç§æ ¼å¼
                                        args = part.function_call.args
                                        if isinstance(args, dict):
                                            function_args = args
                                        elif hasattr(args, 'items'):
                                            # å¤„ç†protobufå­—å…¸ç±»å‹
                                            function_args = dict(args.items())
                                        else:
                                            function_args = dict(args) if args else {}
                                    except Exception as e:
                                        logger.error(f"Error processing function args: {e}")
                                        function_args = {}

                                tool_calls.append({
                                    "id": tool_call_id,
                                    "type": "function",
                                    "function": {
                                        "name": function_name,
                                        "arguments": json.dumps(function_args, ensure_ascii=False)
                                    }
                                })
                                logger.debug(f"Converted function call: {function_name} with args: {list(function_args.keys())}")

                            except Exception as e:
                                logger.error(f"Error processing function call: {e}")
                                continue

            # æ„å»ºæ¶ˆæ¯å¯¹è±¡
            message = {
                "role": "assistant",
                "content": text_content if text_content else None,
            }

            # åªæœ‰åœ¨æœ‰å·¥å…·è°ƒç”¨æ—¶æ‰æ·»åŠ tool_callså­—æ®µ
            if tool_calls:
                message["tool_calls"] = tool_calls
                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ä½†æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œè®¾ç½®finish_reasonä¸ºtool_calls
                if not text_content:
                    finish_reason = "tool_calls"

            choice = {
                "index": 0,
                "message": message,
                "finish_reason": finish_reason
            }

            # å¤„ç†ä½¿ç”¨ç»Ÿè®¡ï¼ˆä¼˜åŒ–ï¼‰
            usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
            if hasattr(gemini_response, 'usage_metadata') and gemini_response.usage_metadata:
                metadata = gemini_response.usage_metadata
                usage = {
                    "prompt_tokens": getattr(metadata, 'prompt_token_count', 0),
                    "completion_tokens": getattr(metadata, 'candidates_token_count', 0),
                    "total_tokens": getattr(metadata, 'total_token_count', 0)
                }

            response = {
                "id": f"chatcmpl-{uuid.uuid4().hex}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [choice],
                "usage": usage,
                "system_fingerprint": f"gemini-{int(time.time())}"
            }

            logger.debug(f"Converted response: finish_reason={finish_reason}, tool_calls={len(tool_calls)}, content_length={len(text_content)}")
            return response

        except Exception as e:
            logger.error(f"Error converting Gemini response: {e}")
            # è¿”å›é”™è¯¯å“åº”
            return {
                "id": f"chatcmpl-{uuid.uuid4().hex}",
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"Error processing response: {str(e)}"
                    },
                    "finish_reason": "stop"
                }],
                "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "system_fingerprint": None
            }

    async def convert_stream_response(self, gemini_stream, original_request: ChatCompletionRequest) -> AsyncGenerator[str, None]:
        """
        å°†Geminiæµå¼å“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼
        ä¿®å¤ï¼šå®Œå…¨é‡æ„äº†å·¥å…·è°ƒç”¨å‚æ•°çš„èšåˆé€»è¾‘ï¼Œä½¿ç”¨å­—å…¸è€Œéå­—ç¬¦ä¸²æ‹¼æ¥
        """
        model = original_request.model
        chat_id = f"chatcmpl-{uuid.uuid4().hex}"
        created_time = int(time.time())

        # ä¿®å¤ï¼šç”¨å­—å…¸è·Ÿè¸ªå·¥å…·è°ƒç”¨çš„å‚æ•°çŠ¶æ€ï¼Œå¹¶è®°å½•ä¸Šæ¬¡å‘é€çš„å­—ç¬¦ä¸²ä»¥è®¡ç®—å¢é‡
        # {tool_call_index: {"id": str, "name": str, "args": dict, "last_sent_str": str}}
        active_tool_calls: Dict[int, Dict] = {}
        tool_call_index_counter = 0
        first_chunk_sent = False
        content_buffer = ""

        try:
            async for chunk in gemini_stream:
                try:
                    if not chunk.candidates:
                        continue
                    
                    candidate = chunk.candidates[0]
                    
                    # å‘é€åˆå§‹è§’è‰²å—
                    if not first_chunk_sent:
                        delta = {"role": "assistant"}
                        choice = {"index": 0, "delta": delta}
                        openai_chunk = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": model,
                            "choices": [choice]
                        }
                        yield f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"
                        first_chunk_sent = True

                    delta = {}
                    # å¤„ç†å†…å®¹
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            # å¤„ç†æ–‡æœ¬å†…å®¹ï¼ˆå¢é‡å‘é€ï¼‰
                            if hasattr(part, 'text') and part.text:
                                new_content = part.text
                                # åªå‘é€æ–°å¢çš„å†…å®¹
                                if len(new_content) > len(content_buffer):
                                    delta_content = new_content[len(content_buffer):]
                                    if delta_content:
                                        delta["content"] = delta_content
                                        content_buffer = new_content

                            # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¢é‡å‘é€ï¼‰
                            if hasattr(part, 'function_call') and part.function_call:
                                func_call = part.function_call
                                func_name = func_call.name
                                
                                # æŸ¥æ‰¾æ­¤å·¥å…·è°ƒç”¨æ˜¯å¦å·²å¼€å§‹
                                current_tool_index = -1
                                for idx, tool in active_tool_calls.items():
                                    if tool["name"] == func_name:
                                        current_tool_index = idx
                                        break

                                # å¦‚æœæ˜¯æ–°çš„å·¥å…·è°ƒç”¨
                                if current_tool_index == -1:
                                    current_tool_index = tool_call_index_counter
                                    tool_call_id = f"call_{uuid.uuid4().hex}"
                                    active_tool_calls[current_tool_index] = {
                                        "id": tool_call_id,
                                        "name": func_name,
                                        "args": {},
                                        "last_sent_str": ""
                                    }
                                    if "tool_calls" not in delta: delta["tool_calls"] = []
                                    delta["tool_calls"].append({
                                        "index": current_tool_index,
                                        "id": tool_call_id,
                                        "type": "function",
                                        "function": {"name": func_name, "arguments": ""}
                                    })
                                    tool_call_index_counter += 1
                                
                                # å®‰å…¨åœ°è·å–å¹¶æ›´æ–°å‚æ•°
                                if hasattr(func_call, 'args') and func_call.args:
                                    # Geminiçš„argsæ˜¯éƒ¨åˆ†æ›´æ–°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åˆå¹¶
                                    new_args = dict(func_call.args.items())
                                    tool_state = active_tool_calls[current_tool_index]
                                    tool_state["args"].update(new_args)

                                    # è®¡ç®—å‚æ•°å­—ç¬¦ä¸²çš„å¢é‡
                                    new_full_args_str = json.dumps(tool_state["args"], ensure_ascii=False, sort_keys=True)
                                    last_sent_str = tool_state["last_sent_str"]
                                    
                                    if len(new_full_args_str) > len(last_sent_str):
                                        arg_delta = new_full_args_str[len(last_sent_str):]
                                        tool_state["last_sent_str"] = new_full_args_str

                                        # å°†å¢é‡æ·»åŠ åˆ°deltaä¸­
                                        if "tool_calls" not in delta: delta["tool_calls"] = []
                                        
                                        tool_delta_found = False
                                        for tc in delta["tool_calls"]:
                                            if tc.get("index") == current_tool_index:
                                                tc["function"]["arguments"] = arg_delta
                                                tool_delta_found = True
                                                break
                                        
                                        if not tool_delta_found:
                                            delta["tool_calls"].append({
                                                "index": current_tool_index,
                                                "function": {"arguments": arg_delta}
                                            })

                    # å‘é€å†…å®¹å—
                    if delta:
                        choice = {"index": 0, "delta": delta}
                        openai_chunk = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": model,
                            "choices": [choice]
                        }
                        yield f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"

                    # å¤„ç†ç»“æŸåŸå› 
                    if hasattr(candidate, 'finish_reason') and candidate.finish_reason and str(candidate.finish_reason) != "FinishReason.FINISH_REASON_UNSPECIFIED":
                        finish_reason = self._map_finish_reason(candidate.finish_reason)
                        final_choice = {"index": 0, "delta": {}, "finish_reason": finish_reason}
                        final_chunk = {
                            "id": chat_id,
                            "object": "chat.completion.chunk",
                            "created": created_time,
                            "model": model,
                            "choices": [final_choice]
                        }
                        yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
                        break

                except Exception as chunk_error:
                    logger.error(f"Error processing stream chunk: {chunk_error}", exc_info=True)
                    continue

        except Exception as stream_error:
            logger.error(f"Fatal error in stream conversion: {stream_error}", exc_info=True)
            # å‘é€æœ€ç»ˆé”™è¯¯å—
            error_chunk = {
                "id": chat_id,
                "object": "chat.completion.chunk",
                "created": created_time,
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": f"\n\n[Stream Error: {str(stream_error)}]"},
                    "finish_reason": "stop"
                }]
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
        finally:
            # å‘é€æµç»“æŸæ ‡å¿—
            yield "data: [DONE]\n\n"
            logger.info(f"Stream finished for chat ID: {chat_id}")

class APIConfig:
    """æŒæœ‰è½¬æ¢å™¨å®ä¾‹çš„é…ç½®ç±»ã€‚"""
    def __init__(self):
        self.openai_to_gemini = OpenAIToGeminiConverter()
        self.gemini_to_openai = GeminiToOpenAIConverter()
        logger.info("ğŸš€ OpenAI-Gemini API Compatibility Layer Initialized (Fixed Version).")
